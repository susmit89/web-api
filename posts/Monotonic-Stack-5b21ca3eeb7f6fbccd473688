{
	"_id ": "5b21ca3eeb7f6fbccd473688",
	"id": "Monotonic-Stack-5b21ca3eeb7f6fbccd473688",
	"title": "Monotonic Stack",
	"topic": {
		"_id": "5b21ca3eeb7f6fbccd486865",
		"name": "Data Structure"
	},
	"numberInStock": 6,
	"dailyRentalRate": 2.5,
	"publishDate": "2020-01-03T19:04:28.809Z",
	"metadata": ["Stack is one of the popular data structures used in a lot of algorithms like backtracking, depth-for-search(DFS), reverse string, undo/redo, call stack and many more. There are many forms of stacks such as Min Stack - which gives the minimum number in stack in constant time O(1). Max Stack - which gives the maximum number. Other than that there is a monotonic stack. It is called monotonic because the order in the stack is either increasing or decreasing."],
	"content": {
		"rows": 7,
		"coumns": 0,
		"images": {},
		"text": [{
				"row": 0,
				"column": 0,
				"type": "heading",
				"value": ["When to use Stack"]
			},
			{
				"row": 1,
				"column": 0,
				"type": "paragraph",
				"value": ["Federated learning (also known as collaborative learning) is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them. This approach stands in contrast to traditional centralized machine learning techniques where all the local datasets are uploaded to one server, as well as to more classical decentralized approaches which often assume that local data samples are identically distributed."]
			},
			{
				"row": 2,
				"column": 0,
				"type": "heading",
				"value": ["Intuition for Monotonic Stack"]
			},
			{
				"row": 3,
				"column": 0,
				"type": "image",
				"caption": ["Your phone personalizes the model locally, based on your usage (A). Many users’ updates are aggregated (B) to form a consensus change © to the shared model, after which the procedure is repeated."],
				"reference": "images/0_yI_rIRNAFTYDwFtK.png"
			},
			{
				"row": 4,
				"column": 0,
				"type": "paragraph",
				"value": ["Federated learning (FL) in contrast, is an approach that downloads the current model and computes an updated model at the device itself (ala edge computing) using local data. These locally trained models are then sent from the devices back to the central server where they are aggregated, i.e. averaging weights, and then a single consolidated and improved global model is sent back to the devices."]
			},
			{
				"row": 5,
				"column": 0,
				"type": "paragraph",
				"value": ["In a more general sense, FL allows for machine learning algorithms to gain experience from a broad range of data sets located at different locations. The approach enables multiple organizations to collaborate on the development of models, but without needing to directly share secure data with each other. Over the course of several training iterations, the shared models get exposed to a significantly wider range of data than what any single organization possesses in-house. In other words, FL decentralizes machine learning by removing the need to pool data into a single location. Instead, the model is trained in multiple iterations at different locations."]
			},
			{
				"row": 6,
				"column": 0,
				"type": "paragraph",
				"value": ["FL enables devices like mobile phones to collaboratively learn a shared prediction model while keeping the training data on the device instead of requiring the data to be uploaded and stored on a central server."]
			},
			{
				"row": 7,
				"column": 0,
				"type": "heading",
				"value": ["How to solve problems usiing Monotonic Stack"]
			},
			{
				"row": 8,
				"column": 0,
				"type": "paragraph",
				"value": ["It works like this: your device downloads the current model, improves it by learning from data on your phone, and then summarizes the changes as a small focused update. Only this update to the model is sent to the cloud, using encrypted communication, where it is immediately averaged with other user updates to improve the shared model. All the training data remains on your device, and no individual updates are stored in the cloud."]
			},
			{
				"row": 9,
				"column": 0,
				"type": "heading",
				"value": ["Where to use Montonic Stack"]
			},
			{
				"row": 10,
				"column": 0,
				"type": "paragraph",
				"value": ["There are a number of core challenges associated with FL. First, communication is a critical bottleneck in FL networks where data generated on each device remain local. In order to train a model using data generated by the devices in the network, it is necessary to develop communication-efficient methods that reduce the total number of communication rounds, and also iteratively send small model updates as part of the training process, as opposed to sending the entire data set."]
			}
		]
	}
}
