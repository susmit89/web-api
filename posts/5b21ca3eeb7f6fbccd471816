{
	"_id ": "5b21ca3eeb7f6fbccd471816",
	"id": "Federated-Learning-5b21ca3eeb7f6fbccd471816",
	"title": "Federated Learning",
	"topic": {
		"_id": "5b21ca3eeb7f6fbccd471818",
		"name": "Artificial Intelligence"
	},
	"numberInStock": 6,
	"dailyRentalRate": 2.5,
	"publishDate": "2018-01-03T19:04:28.809Z",
	"metadata": ["Federated learning (FL) in contrast, is an approach that downloads the current model and computes an updated model at the device itself (ala edge computing) using local data. These locally trained models are then sent from the devices back to the central server where they are aggregated, i.e. averaging weights, and then a single consolidated and improved global model is sent back to the devices."],
	"content": {
		"rows": 7,
		"coumns": 0,
		"images": {},
		"text": [{
				"row": 0,
				"column": 0,
				"type": "heading",
				"value": ["Background"]
			},
			{
				"row": 1,
				"column": 0,
				"type": "paragraph",
				"value": ["Federated learning (also known as collaborative learning) is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them. This approach stands in contrast to traditional centralized machine learning techniques where all the local datasets are uploaded to one server, as well as to more classical decentralized approaches which often assume that local data samples are identically distributed."]
			},
			{
				"row": 2,
				"column": 0,
				"type": "paragraph",
				"value": ["Federated learning enables multiple actors to build a common, robust machine learning model without sharing data, thus allowing to address critical issues such as data privacy, data security, data access rights and access to heterogeneous data. Its applications are spread over a number of industries including defense, telecommunications, IoT, and pharmaceutics."]
			},
			{
				"row": 3,
				"column": 0,
				"type": "image",
				"caption": ["Your phone personalizes the model locally, based on your usage (A). Many users’ updates are aggregated (B) to form a consensus change © to the shared model, after which the procedure is repeated."],
				"reference": "/images/0*yI_rIRNAFTYDwFtK.png"
			},
			{
				"row": 4,
				"column": 0,
				"type": "paragraph",
				"value": ["Federated learning (FL) in contrast, is an approach that downloads the current model and computes an updated model at the device itself (ala edge computing) using local data. These locally trained models are then sent from the devices back to the central server where they are aggregated, i.e. averaging weights, and then a single consolidated and improved global model is sent back to the devices."]
			},
			{
				"row": 5,
				"column": 0,
				"type": "paragraph",
				"value": ["In a more general sense, FL allows for machine learning algorithms to gain experience from a broad range of data sets located at different locations. The approach enables multiple organizations to collaborate on the development of models, but without needing to directly share secure data with each other. Over the course of several training iterations, the shared models get exposed to a significantly wider range of data than what any single organization possesses in-house. In other words, FL decentralizes machine learning by removing the need to pool data into a single location. Instead, the model is trained in multiple iterations at different locations."]
			},
			{
				"row": 6,
				"column": 0,
				"type": "paragraph",
				"value": ["FL enables devices like mobile phones to collaboratively learn a shared prediction model while keeping the training data on the device instead of requiring the data to be uploaded and stored on a central server."]
			},
			{
				"row": 7,
				"column": 0,
				"type": "heading",
				"value": ["Google describes how FL works in this way with respect to mobile phones:"]
			},
			{
				"row": 8,
				"column": 0,
				"type": "paragraph",
				"value": ["It works like this: your device downloads the current model, improves it by learning from data on your phone, and then summarizes the changes as a small focused update. Only this update to the model is sent to the cloud, using encrypted communication, where it is immediately averaged with other user updates to improve the shared model. All the training data remains on your device, and no individual updates are stored in the cloud."]
			},
			{
				"row": 9,
				"column": 0,
				"type": "heading",
				"value": ["Challenges"]
			},
			{
				"row": 10,
				"column": 0,
				"type": "paragraph",
				"value": ["There are a number of core challenges associated with FL. First, communication is a critical bottleneck in FL networks where data generated on each device remain local. In order to train a model using data generated by the devices in the network, it is necessary to develop communication-efficient methods that reduce the total number of communication rounds, and also iteratively send small model updates as part of the training process, as opposed to sending the entire data set."]
			}
		]
	}
}
